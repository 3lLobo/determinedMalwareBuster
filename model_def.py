from determined.pytorch import DataLoader, PyTorchTrial, PyTorchTrialContext
import torch
from torch.utils import data
from torch import nn
import pandas as pd
from src.mlp import Mlp
from src.data import MalwareDataset
from sklearn.model_selection import train_test_split
from src.data import drop_nonnumeric
from typing import Any, Dict, Sequence, Tuple, Union

SK_SEED = 11

view_link = (
    "https://drive.google.com/file/d/1ot3t_5cAP2L3zP_tqK-1h2EM4WyMCeAI/view?usp=sharing"
)
file_id = view_link.split("/")[-2]
drive_link = (
    f"https://docs.google.com/uc?export=download&id=1ot3t_5cAP2L3zP_tqK-1h2EM4WyMCeAI"
)

# target = "malware_type_family"

# TODO: define data loading.
# TUTORIAL: https://docs.determined.ai/latest/tutorials/pytorch-mnist-tutorial.html


class DeterminedNet(PyTorchTrial):
    def __init__(self, context: PyTorchTrialContext, device: torch.device = "cuda"):
        # Store trial context for later use.
        self.context = context
        self.device = device

        # Create a unique download directory for each rank so they don't overwrite each
        # other when doing distributed training.
        # self.download_directory = f"/tmp/data-rank{self.context.distributed.get_rank()}"
        self.download_directory = f"./tmp"
        # Profile CPU and GPOU usage.
        context.set_profiler(
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
            schedule=torch.profiler.schedule(
                wait=1,
                warmup=1,
                active=2,
                repeat=2,
            ),
        )
        # df = pd.read_csv(drive_link)
        df = pd.read_csv("data/datasetMalMem22.csv")
        target = self.context.get_hparam("target_label")
        df = drop_nonnumeric(df, target, normalize=True)

        df_train, df_test = train_test_split(df, test_size=0.2, random_state=11)

        self.dataset_train = MalwareDataset(df_train, target, normalize=True)
        self.dataset_test = MalwareDataset(df_test, target, normalize=True)

        self.data_downloaded = True

        n_dims, n_classes = self.dataset_train.get_dimensions()

        # Wrap things up
        self.model = Mlp(
            input_size=n_dims,
            output_size=n_classes,
            dropout=self.context.get_hparam("dropout1"),
        ).to(device)
        if self.context.wrap_model is not None:
            self.model = self.context.wrap_model(self.model)

        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=context.get_hparam("learning_rate"),
        )
        if self.context.wrap_optimizer is not None:
            self.optimizer = self.context.wrap_optimizer(self.optimizer)

        self.loss = nn.NLLLoss()
        try:
            if self.context.wrap_loss is not None:
                self.loss = self.context.wrap_loss(self.loss)
        except AttributeError as e:
            pass
        finally:
            pass

    def build_training_data_loader(self) -> DataLoader:
        data_loader = self.dataset_train.get_dataloader(
            batch_size=self.context.get_per_slot_batch_size(),
            shuffle=True,
            num_workers=1,
        )
        return data_loader

    def build_validation_data_loader(self) -> DataLoader:
        data_loader = self.dataset_test.get_dataloader(
            batch_size=self.context.get_per_slot_batch_size(),
            shuffle=True,
            num_workers=1,
        )
        return data_loader

    def train_batch(self, batch: Any, epoch_idx: int, batch_idx: int) -> Dict[str, Any]:
        # Forward pass.
        x, y = batch
        x, y = x.to(self.device), y.to(self.device)
        logits = self.model(x)

        # Print the values.
        # print(f"Epoch: {epoch_idx}, Batch: {batch_idx}")
        # print(f"Logits: {logits} shape: {logits.shape}")
        # print(f"Data: {x}")
        # print(f"Labels: {y} shape: {y.shape}")

        loss = self.loss(logits, y)

        # Backward pass.
        try:
            self.context.backward(loss)
            self.context.step_optimizer(self.optimizer)
        except:
            with torch.no_grad():
                loss.backward()
                self.optimizer.step()

        # Calculate accuracy.
        y_pred = torch.argmax(logits, axis=1)
        acc = (y_pred == y).sum().item() / len(y)

        self.current_epoch = epoch_idx
        self.val_steps = 0
        # Print an update.
        if batch_idx % 10 == 0:
            print(
                f"Epoch: {epoch_idx}, Batch: {batch_idx}, Loss: {loss.item()}, Accuracy: {acc}"
            )
        return {
            "taining_loss": loss.item(),
            "taining_acc": acc,
            # "epoch": epoch_idx,
            "batch": batch_idx,
        }

    def evaluate_batch(self, batch: Any) -> Dict[str, Any]:
        x, y = batch
        x, y = x.to(self.device), y.to(self.device)
        logits = self.model(x)
        loss = self.loss(logits, y)

        y_pred = torch.argmax(logits, axis=1)
        acc = (y_pred == y).sum().item() / len(y)

        # Print an update.
        print(f"Validation Loss: {loss.item()}, Accuracy: {acc}")
        self.val_steps += len(y)

        return {
            "validation_loss": loss.item(),
            "test_loss": loss.item(),
            "validation_acc": acc,
            # "epoch": self.current_epoch,
            "batch": self.val_steps,
        }
