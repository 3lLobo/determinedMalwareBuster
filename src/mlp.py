# Simple MLP implementation in PyTorch

import torch.nn as nn
import numpy as np


class Mlp(nn.Module):
    def __init__(
        self, input_size: int, output_size: int, dropout: int = 0.2, n_layers: int = 1
    ):
        """Initialize the MLP model

        Args:
            input_size (int): The size of the input vector.
            output_size (int): The size of the output vector.
            dropout (int, optional): The dropout rate. Defaults to 0.2.
            n_layers (int, optional): The number of hidden layers. Defaults to 1.
        """
        super(Mlp, self).__init__()

        layers = []
        d_hidden = [int(i) for i in np.linspace(input_size, output_size, n_layers + 2)]
        for i in range(len(d_hidden) - 1):
            layers.append(nn.Linear(d_hidden[i], d_hidden[i + 1]))
            if i < n_layers - 2:
                layers.append(nn.ReLU())
                layers.append(nn.Dropout(dropout))
            else:
                layers.append(nn.LogSoftmax())

        self.model = nn.Sequential(*layers)
        self.flatten = nn.Flatten()

    def forward(self, x):
        x = self.flatten(x)
        return self.model(x)
